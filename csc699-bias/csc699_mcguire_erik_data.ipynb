{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of csc699-mcguire_erik-data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Z0pQKleSe8o-",
        "gouHaOAlvkBt",
        "QGuTqiBXglws",
        "1o7epH0fV4wq",
        "ceslQ9RMV7dw",
        "9dahdZ4CokZO",
        "WjhZ2903uCsp",
        "yNgeeTnCuKsh",
        "9LDpmV_rEArr",
        "odZBcYA7ixEt",
        "e8OfX0FP1UAg",
        "7HEghfc0LrOE",
        "UX6fo9GlfM6b",
        "9LK91hmIUwB9",
        "RIfm4WTjUyad",
        "GKUpneo8WFgl",
        "7TxV_boQWvHR"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0pQKleSe8o-",
        "colab_type": "text"
      },
      "source": [
        "# AMT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gouHaOAlvkBt",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCQmvsPIjxJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import numpy as np\n",
        "import random\n",
        "from itertools import combinations, permutations\n",
        "import warnings\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "sns.set_style('white') \n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGuTqiBXglws",
        "colab_type": "text"
      },
      "source": [
        "## Inter-rater reliability\n",
        "\n",
        "For each sample, there are 3 rows with annotation from each annotator.\n",
        "\n",
        "Before selecting majority vote, remove intersectional samples and compute inter-annotator agreement using Fleiss' kappa (appropriate for different raters comprising the 3 per sample) and Spearman's correlation (treating valences as ordinal) by APIAA/AMIAA methods, modified to suit crowd annotation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o7epH0fV4wq",
        "colab_type": "text"
      },
      "source": [
        "### Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4tLkdcI0MNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_turkSR(filename: str = \"my_turk_annotated.csv\", df: bool = False):\n",
        "    \"\"\"Define and clean dataframe of AMT results.\"\"\"\n",
        "    # Extract and numericalize pertinent columns from MTurk results.\n",
        "    turkdf = pd.read_csv(f\"drive/My Drive/csc699/{filename}\", # AMT results .csv\n",
        "                        converters={\"Answer.regard.label\": lambda x: x.replace(\"Neutral\", \"0\")\n",
        "                                                                      .replace(\"Positive\", \"1\")\n",
        "                                                                      .replace(\"Negative\", \"-1\")\n",
        "                                                                      .replace(\"N/A\", \"2\")})\n",
        "    if not df:\n",
        "        turkSR = turkdf.iloc[:, 27:29] # Extract Sample, Regard columns.\n",
        "        turkSR = turkSR.rename(columns={\"Input.text\": \"Sample\", \n",
        "                                        \"Answer.regard.label\": \"Regard\"})\n",
        "    else:\n",
        "        turkSR = turkdf\n",
        "    return turkSR\n",
        "    \n",
        "def remove_intersectional(df = None, is_vader=False, is_intrs: bool = False):\n",
        "    \"\"\"\n",
        "    Retrieve intersectional samples to use \n",
        "    in reference dataframe for filtering main AMT dataframe.\n",
        "    \"\"\"\n",
        "    turkSR = df if is_vader else get_turkSR()\n",
        "\n",
        "    intersectional_samples_df = pd.read_csv(\"drive/My Drive/csc699/pre_annotated_int.csv\", \n",
        "                                             header=None,\n",
        "                                             sep=\"\\t\")\n",
        "        \n",
        "    if not is_intrs:\n",
        "        # Get intersectional indices to drop.\n",
        "        intersectional_indices = turkSR[turkSR.Sample.apply(lambda x: \n",
        "                                                            x in intersectional_samples_df[0].values)].index\n",
        "    else:\n",
        "        # Get non-intersectional indices to drop (or: keeping intersectional).\n",
        "        intersectional_indices = turkSR[turkSR.Sample.apply(lambda x: \n",
        "                                                            x not in intersectional_samples_df[0].values)].index\n",
        "    turkSR.drop(intersectional_indices, inplace=True) # Remove intersectional entries.\n",
        "    turkSR.Regard = turkSR.Regard.apply(lambda reg: int(reg)) # Convert string score to integer.\n",
        "    return turkSR\n",
        "\n",
        "def fleiss_kappa(ratings: list = [], n: int = 3, k: int = 3):\n",
        "    '''\n",
        "    https://gist.github.com/ShinNoNoir/4749548\n",
        "    Computes the Fleiss' kappa measure for assessing the reliability of \n",
        "    agreement between a fixed number n of raters when assigning categorical\n",
        "    ratings to a number of items.\n",
        "    \n",
        "    Args:\n",
        "        ratings: a list of tuples [(item1, category2), (item1, category1) ...]\n",
        "        n: number of raters\n",
        "        k: number of categories\n",
        "    Returns:\n",
        "        the Fleiss' kappa score\n",
        "    \n",
        "    See also:\n",
        "        http://en.wikipedia.org/wiki/Fleiss'_kappa\n",
        "    '''\n",
        "    items = set()\n",
        "    categories = set()\n",
        "    n_ij = {}\n",
        "    \n",
        "    for i, c in ratings:\n",
        "        if c != 2:\n",
        "            items.add(i)\n",
        "            categories.add(c)\n",
        "            n_ij[(i,c)] = n_ij.get((i,c), 0) + 1\n",
        "    \n",
        "    N = len(items)\n",
        "    \n",
        "    p_j = {}\n",
        "    for c in categories:\n",
        "        p_j[c] = sum(n_ij.get((i,c), 0) for i in items) / (1.0*n*N)\n",
        "    \n",
        "    P_i = {}\n",
        "    for i in items:\n",
        "        P_i[i] = (sum(n_ij.get((i,c), 0)**2 for c in categories)-n) / (n*(n-1.0))\n",
        "    \n",
        "    P_bar = sum(P_i.values()) / (1.0*N)\n",
        "    P_e_bar = sum(p_j[c]**2 for c in categories)\n",
        "    \n",
        "    kappa = (P_bar - P_e_bar) / (1 - P_e_bar)\n",
        "    \n",
        "    return kappa\n",
        "\n",
        "def get_combo_df(is_intrs: bool = False):\n",
        "    turkSIDR = get_turkSR(df=True)\n",
        "    turkSIDR = turkSIDR.rename(columns={'Answer.regard.label': 'Regard', \n",
        "                                        'Input.text': 'Sample'})\n",
        "    if not is_intrs:\n",
        "        turkSIDR = remove_intersectional(df=turkSIDR, \n",
        "                                         is_vader=True,\n",
        "                                         is_intrs = False)\n",
        "    else:\n",
        "        turkSIDR = remove_intersectional(df=turkSIDR, \n",
        "                                         is_vader=True,\n",
        "                                         is_intrs = True)\n",
        "    keep = ['WorkerId', 'Regard', 'Sample']\n",
        "    turkSIDR_keep = pd.DataFrame()\n",
        "    for i in list(turkSIDR):\n",
        "        if i in keep:\n",
        "            turkSIDR_keep[i] = turkSIDR[i]\n",
        "    turkSIDR_keep.reset_index(level=0, inplace=True)\n",
        "    return turkSIDR_keep\n",
        "    \n",
        "def get_combo_dict(turk_df):\n",
        "    df = pd.pivot_table(turk_df, \n",
        "                        index='Sample', \n",
        "                        columns='WorkerId', \n",
        "                        values='Regard')\n",
        "    worker_ids = list(set(turk_df.WorkerId))\n",
        "    wid_dict = {wid: dict() for wid in worker_ids}\n",
        "    for worker_id in worker_ids:\n",
        "        samples = list(turk_df.Sample[turk_df.WorkerId == worker_id])\n",
        "        for sample in samples:\n",
        "            sample_df = pd.DataFrame(df.loc[sample]).T.dropna(axis=1)\n",
        "            tups = [(wid, sample_df[wid][0]) for wid in list(sample_df)]\n",
        "            scores = [tup[1] for tup in tups]\n",
        "            if 2 not in scores: # Remove N/A.\n",
        "                combos = combinations(tups, 2)\n",
        "                combos = list(filter(lambda x: worker_id in [x[0][0], x[1][0]], combos))\n",
        "                wid_dict[worker_id][sample] = combos\n",
        "            else:\n",
        "                continue\n",
        "    return wid_dict\n",
        "\n",
        "def get_worker_agreement_percentage(wid_dict: dict,\n",
        "                                    worker_id: str,\n",
        "                                    judges: int = 3):\n",
        "    \"\"\"\n",
        "        Compute basic agreement.\n",
        "    \"\"\"\n",
        "    cnt = 0\n",
        "    pairs = wid_dict[worker_id].values()\n",
        "    score_pairs = map(lambda x: (x[0][0][1], x[0][1][1]), # 1st, 2nd scores\n",
        "                      pairs)\n",
        "    for scores in score_pairs:\n",
        "        if scores[0] == scores[1]:\n",
        "            cnt += 1\n",
        "    print(f'Worker {worker_id} agrees with the other {judges-1} annotators', \n",
        "          f'{round(cnt/len(pairs) * 100)}% of the time.\\n')\n",
        "\n",
        "def get_id_dict(df, \n",
        "                wid_dict: dict,\n",
        "                worker_id: str):\n",
        "    \"\"\"\n",
        "    Return dictionary of ids, scores\n",
        "    for each judge worked with in annotations dataset.\n",
        "    \"\"\"\n",
        "    ids = pd.Series({id_: [] for id_ in df.WorkerId.values})\n",
        "    for sample, id_score_pairs in wid_dict[worker_id].items():\n",
        "        # sample, [(id_, score), (p_id, score)]\n",
        "        id_score_pairs = id_score_pairs[0]\n",
        "        partner_id = id_score_pairs[0][0]\n",
        "        main_id = id_score_pairs[1][0]\n",
        "        partner_score = id_score_pairs[0][1]\n",
        "        main_score = id_score_pairs[1][1]\n",
        "        if partner_id != worker_id:\n",
        "            ids[partner_id].append((partner_score, main_score))\n",
        "        elif main_id != worker_id:\n",
        "            ids[main_id].append((partner_score, main_score))\n",
        "    return {k: v for k, v in ids.items() if v}, ids\n",
        "\n",
        "def get_id_apiaa(id_dict: dict):\n",
        "    warnings.filterwarnings(\"ignore\", \n",
        "                            category=RuntimeWarning) \n",
        "    rhos_ = []\n",
        "    for partner_id, score_pairs in id_dict.items():\n",
        "        \"\"\"\n",
        "        score_pairs: rows = samples/observations, \n",
        "                     cols = each judge's scores/variables\n",
        "                     (assuming spearmanr axis=0)\n",
        "        \"\"\"\n",
        "        # Correlation for all pairs co-scored with this partner.\n",
        "        if len(score_pairs) > 1:\n",
        "            rho, pval = stats.spearmanr(score_pairs, \n",
        "                                        axis=0, \n",
        "                                        nan_policy='omit')\n",
        "            if not np.isnan(rho):\n",
        "                rhos_.append(rho)\n",
        "    n = len(rhos_) # Accounting for nan changing number of annotators.\n",
        "    if n > 1:\n",
        "        sigma = sum(rhos_) # Sum correlations pairwise.\n",
        "        apiaa = sigma/n\n",
        "    else:\n",
        "        apiaa = 0\n",
        "    warnings.filterwarnings(\"default\", category=RuntimeWarning) \n",
        "    return apiaa, rhos_\n",
        "\n",
        "def compute_apiaa(turk_df, wid_dict, judges: int = 3):\n",
        "    \"\"\"\n",
        "    Average Pairwise Inter-Annotator Agreement\n",
        "    APIAA: https://arxiv.org/pdf/2003.04866.pdf\n",
        "    AKA IAA-1: https://www.aclweb.org/anthology/D16-1235.pdf\n",
        "    \"\"\"\n",
        "    apiaa_sum = 0\n",
        "    apiaa_series = pd.Series(zip([], list(wid_dict)))\n",
        "    n = len(wid_dict)\n",
        "    for worker_id in list(wid_dict):\n",
        "        ids, _ = get_id_dict(df=turk_df, \n",
        "                          wid_dict=wid_dict, \n",
        "                          worker_id=worker_id)\n",
        "        apiaa_i, rhos_ = get_id_apiaa(ids)\n",
        "        apiaa_sum += apiaa_i\n",
        "        apiaa_series[worker_id] = rhos_\n",
        "    return apiaa_sum/n, apiaa_series\n",
        "\n",
        "def get_amiaa(turk_df, wid_dict):\n",
        "    \"\"\"\n",
        "    Average Mean Inter-Annotator Agreement\n",
        "    AMIAA: https://arxiv.org/pdf/2003.04866.pdf\n",
        "    AKA IAA-2: https://www.aclweb.org/anthology/D16-1235.pdf\n",
        "    \"\"\"\n",
        "    worker_ids = list(wid_dict)\n",
        "    amiaa = 0\n",
        "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
        "    for i in worker_ids:\n",
        "        mu_i = []\n",
        "        s_i = []\n",
        "        ids, _ = get_id_dict(df=turk_df, wid_dict=wid_dict, worker_id=i)\n",
        "        for sample, score_pairs in wid_dict[i].items():\n",
        "                # Compute mean of co-judge scores for this sample.\n",
        "                # By number of co-judges (i.e., 2).\n",
        "                mu = sum([pair[1][1] \n",
        "                        for pair in score_pairs\n",
        "                        if pair[1][0] != i])/(len(score_pairs))\n",
        "                # Extract judge i's score for this sample.\n",
        "                s = [judge[1] \n",
        "                        for pair in score_pairs \n",
        "                        for judge in pair \n",
        "                        if judge[0] == i][0]\n",
        "                s_i.append(s)\n",
        "                mu_i.append(mu)\n",
        "        rho_i, _ = stats.spearmanr(s_i, mu_i) # Correlate i's scores for i's samples w/ avg of partner ratings.\n",
        "        if not np.isnan(rho_i): # NaN likely due to no variance over small # of samples.\n",
        "            # Average the mean IAA by total number of annotators.\n",
        "            amiaa += rho_i/len(wid_dict)\n",
        "    warnings.filterwarnings(\"default\", category=RuntimeWarning) \n",
        "    return amiaa\n",
        "\n",
        "def display_wid_mat(worker_id, turk_df):\n",
        "    df = pd.pivot_table(turk_df, \n",
        "                        index='Sample', \n",
        "                        columns='WorkerId', \n",
        "                        values='Regard')\n",
        "    #query = 'XYZ was well-known'\n",
        "    #ix = list(map(lambda k: k[:len(query)], df.T.keys())).index(query)\n",
        "    samples = list(turk_df.Sample[turk_df.WorkerId == worker_id])\n",
        "    ndf = ''\n",
        "    for ix, sample in enumerate(samples):\n",
        "        try:\n",
        "            sample_df = pd.DataFrame(df.loc[sample]).T.dropna(axis=1)\n",
        "            fdf = (pd.DataFrame(df.loc[sample])\n",
        "                    .T.dropna(axis=1))\n",
        "            fdf = (fdf.reset_index()\n",
        "            .rename(columns={'index': 'Sample'})\n",
        "            .reset_index()\n",
        "            .rename_axis(None, axis=1).drop('index', axis=1))\n",
        "            if ix == 0:\n",
        "                ndf = fdf\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            if list(fdf):\n",
        "                if ix > 0:\n",
        "                    ndf = pd.concat([ndf, fdf])\n",
        "        except:\n",
        "            pass\n",
        "    ndf = (ndf.reset_index()\n",
        "              .rename(columns={'index': 'Sample'})\n",
        "              .reset_index()\n",
        "              .rename_axis(None, axis=1).drop('index', axis=1))\n",
        "\n",
        "\n",
        "    ndf = pd.DataFrame(ndf.T.iloc[1:]).fillna('Â°')\n",
        "    return ndf\n",
        "\n",
        "def get_masked_clear_df(clear: str = \"sampledv2.csv\"):\n",
        "    \"\"\"Load XYZ, unmasked Turk data.\"\"\"\n",
        "    clear_df = pd.read_csv(f\"drive/My Drive/csc699/{clear}\") #No intersectional.\n",
        "    clear_df = pd.DataFrame({\"Sample\": [s[:-2].rstrip().replace(\"\\\"\", \"'\")\n",
        "                                        for i in clear_df.loc[0, :] \n",
        "                                        for s in list(eval(i))]})\n",
        "    return clear_df\n",
        "\n",
        "def get_mask_clear_lst(ndf, \n",
        "                       clear: str = \"sampledv2.csv\"):\n",
        "    \"\"\"\n",
        "    Create lists for conversion \n",
        "    and combination into dataframe.\n",
        "    \"\"\"\n",
        "    clear_df = get_masked_clear_df(clear)\n",
        "    masklst = ndf.T.Sample.values.tolist()\n",
        "    clearlst = clear_df.Sample.values.tolist()\n",
        "    ndf_sample = pd.Series(ndf.T.Sample)\n",
        "    for clear_sample in clearlst:\n",
        "        for ix, xyz_sample in enumerate(masklst):\n",
        "            if xyz_sample[4:] in clear_sample:\n",
        "                ndf.iloc[0, ix] = clear_sample\n",
        "                break\n",
        "    ndf = ndf.T\n",
        "    ndf.insert(value=ndf_sample, column='Sample_XYZ', loc=1)\n",
        "    ndf = ndf.T\n",
        "    return ndf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceslQ9RMV7dw",
        "colab_type": "text"
      },
      "source": [
        "### Get APIAA, AMIAA, Fleiss' kappa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjG10K9bEaAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "turk_df = get_combo_df(is_intrs = False)\n",
        "wid_dict = get_combo_dict(turk_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q236hX7I0feX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#worker_id = random.choice(turk_df.WorkerId.values)\n",
        "worker_id = \"A3E18P15E4GSG6\"\n",
        "\n",
        "ids, ids_series = get_id_dict(df=turk_df, wid_dict=wid_dict, worker_id=worker_id)\n",
        "apiaa, rhos_ = get_id_apiaa(ids)\n",
        "n = len(rhos_)\n",
        "get_worker_agreement_percentage(wid_dict=wid_dict, worker_id=worker_id, judges=3)\n",
        "#display(wid_dict[worker_id][random.choice(list(list(wid_dict[worker_id])))])\n",
        "if n > 0:\n",
        "    print(f'\\nAverage pairwise correlation for samples worked on by {worker_id}: {sum(rhos_)/n}.\\n')\n",
        "#display(wid_dict[worker_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp-K4BSETMpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids_df = pd.DataFrame(ids_series)\n",
        "ids_df[ids_df[0].str.len() != 0]\n",
        "pd.DataFrame(wid_dict[worker_id]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxySEu3OnJxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "apiaa, apiaa_series = compute_apiaa(turk_df, wid_dict, judges=3)\n",
        "print('\\nAverage pairwise Spearman\\'s correlation (APIAA) for all workers with partitioned dataset:\\n', \n",
        "      round(apiaa, 2), '\\n')\n",
        "#display(apiaa_series[worker_id])\n",
        "amiaa = get_amiaa(turk_df, wid_dict)\n",
        "print('\\nAverage Mean Spearman\\'s correlation (AMIAA):\\n', \n",
        "      round(amiaa, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mR3CJEuuEE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "turkSR = remove_intersectional()\n",
        "print(\"Fleiss' kappa:\\n\", fleiss_kappa(turkSR.values.tolist()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pojiZn-QhTuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "worker_id = random.choice(turk_df.WorkerId.values)\n",
        "#worker_id = 'A3E18P15E4GSG6'\n",
        "print(worker_id, \"\\n\")\n",
        "ndf = display_wid_mat(worker_id, turk_df).iloc[:, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGXCFH8M7vq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(ndf)\n",
        "ndf = ndf.loc[:, ~(ndf == 2).any()]\n",
        "ndf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6hk3na6CgOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_mask_clear_lst(ndf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dahdZ4CokZO",
        "colab_type": "text"
      },
      "source": [
        "## VADER vs. majority regard correlations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9pCJe1zpLjc",
        "colab_type": "text"
      },
      "source": [
        "Compress three annotators' scores to majority."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU0xbJNRIvVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vote(arr):\n",
        "    \"\"\"Choose majority score or discard.\"\"\"\n",
        "    arr = arr.tolist()\n",
        "    c1 = (0, arr.count(0))\n",
        "    c2 = (-1, arr.count(-1))\n",
        "    c3 = (1, arr.count(1))\n",
        "    if c1[1] == c2[1] == c3[1]:\n",
        "        # Choose random if tied.\n",
        "        # v = random.choice(arr)\n",
        "        v = None # Discard if tied.\n",
        "    else: # No tie.\n",
        "        v = sorted([c1, c2, c3], # Retrieve score/most votes.\n",
        "                   key=lambda x: -x[1])[0][0]\n",
        "    return v\n",
        "\n",
        "def get_majority_df(intersectional: bool = False):\n",
        "    \"\"\"Return turkSR with majority scores.\"\"\"\n",
        "    if intersectional:\n",
        "        turkSR = remove_non_intersectional()\n",
        "    else:\n",
        "        turkSR = remove_intersectional()\n",
        "    reference_df = pd.DataFrame(turkSR.groupby(\"Sample\", \n",
        "                                               sort=False).aggregate({\"Regard\": vote}))\n",
        "    reference_df.reset_index(level=0, inplace=True) # Fix headers.\n",
        "    reference_df.dropna(inplace=True) # Remove ties.\n",
        "    reference_df.Regard = reference_df.Regard.apply(lambda x: int(x))\n",
        "    return reference_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PNtLR2LMU78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reference_df = get_majority_df()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjhZ2903uCsp",
        "colab_type": "text"
      },
      "source": [
        "### Write formatted AMT to file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT1VCGyOkBMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "remove_intersectional().to_csv(\"drive/My Drive/csc699/my_turk_no_intersectional.csv\", index=False)\n",
        "#remove_non_intersectional().to_csv(\"drive/My Drive/csc699/my_turk_intersectional.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SraefmorU0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_majority_df().to_csv(\"drive/My Drive/csc699/my_turk_majority_no_ties.csv\", index=False)\n",
        "#get_majority_df(True).to_csv(\"drive/My Drive/csc699/my_turk_majority_int_no_ties.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNgeeTnCuKsh",
        "colab_type": "text"
      },
      "source": [
        "### Compare VADER and human scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzP92zGYBWMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vader_df():\n",
        "    \"\"\"Get dataframe for VADER sentiment scores.\"\"\"\n",
        "    vader_samples = pd.read_csv(\"drive/My Drive/csc699/combined_ano_xyz.csv\")\n",
        "    vader_labels = pd.read_csv(\"drive/My Drive/csc699/combined_ano_labs.csv\")\n",
        "    vader_samples = vader_samples.rename(columns={\"text\": \"Sample\"})\n",
        "    vader_labels = vader_labels.rename(columns={\"text\": \"Regard\"})\n",
        "    vader_df = pd.concat([vader_samples, vader_labels], axis=1)\n",
        "    vader_df = remove_intersectional(vader_df, is_vader=True)\n",
        "    return vader_df\n",
        "\n",
        "def get_vader_turk_lists():\n",
        "    \"\"\"Create lists for comparison of sentiment, regard.\"\"\"\n",
        "    regard_df = get_majority_df()\n",
        "    vader_df = get_vader_df()\n",
        "    regardlst = regard_df.Sample.values.tolist()\n",
        "    xyzlst = vader_df.values.tolist()\n",
        "    vaderlst = list(filter(lambda sr: sr[0] in regardlst, xyzlst))\n",
        "    turklst = regard_df.values.tolist()\n",
        "    return (sorted(vaderlst, key=lambda t: t[0]), \n",
        "            sorted(turklst, key=lambda t: t[0]))\n",
        "\n",
        "def print_vader_turk(n: int = None):\n",
        "    \"\"\"Display paired sentences with respective scores.\"\"\"\n",
        "    vaderlst, turklst = get_vader_turk_lists()\n",
        "    n = n or len(vaderlst)\n",
        "    print(\"\\n\")\n",
        "    for turk, vader in zip(turklst[:n], vaderlst[:n]):\n",
        "        print(f\"{turk[0]}\\n  MTurk Regard: {turk[1]}\\n  VADER Sentiment: {vader[1]}\")\n",
        "\n",
        "def show_regard_props():\n",
        "    temp = get_majority_df()\n",
        "    print('# Positive: ', temp.Regard[temp.Regard > 0].count())\n",
        "    print('# Neutral: ', temp.Regard[temp.Regard == 0].count())\n",
        "    print('# Negative: ', temp.Regard[temp.Regard < 0].count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s1pkc2STjAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_regard_props()\n",
        "# print_vader_turk(n = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnkAS2B4rFdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_sent_reg_corr():\n",
        "    both = []\n",
        "    respect = []\n",
        "    occupation = []\n",
        "    vaderlst, turklst = get_vader_turk_lists()\n",
        "    for turk, vader in zip(turklst, vaderlst):\n",
        "        both.append((turk[1], vader[1]))\n",
        "        if \"XYZ was\" in turk[0]:\n",
        "            respect.append((turk[1], vader[1]))\n",
        "        else:\n",
        "            occupation.append((turk[1], vader[1]))\n",
        "\n",
        "    r, p = stats.spearmanr(both)\n",
        "    respect_rho, respect_pval = stats.spearmanr(respect)\n",
        "    occupation_rho, occupation_pval = stats.spearmanr(occupation)\n",
        "    print('VADER vs. Regard (Both) rho:\\n', r)\n",
        "    print('VADER vs. Regard (Both) p-value:\\n', p)\n",
        "    print(\"\\n___________\\n\")\n",
        "    print('VADER vs. Regard (Respect) rho:\\n', respect_rho)\n",
        "    print('VADER vs. Regard (Occupation) rho:\\n', occupation_rho)\n",
        "    print(\"\\n___________\\n\")\n",
        "    print('VADER vs. Regard (Respect) p-value:\\n', respect_pval)\n",
        "    print('VADER vs. Regard (Occupation) p-value:\\n', occupation_pval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F51XdkazvJhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_sent_reg_corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LDpmV_rEArr",
        "colab_type": "text"
      },
      "source": [
        "# Significance Tests (Monte Carlo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odZBcYA7ixEt",
        "colab_type": "text"
      },
      "source": [
        "#### Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKdfAbZa_C_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_score(y_true, y_pred):\n",
        "    \"\"\"Test statistic for classifier permutation test.\"\"\"\n",
        "    corr = sum(y_t==y_p for y_t, y_p in zip(y_true, y_pred))\n",
        "    n = len(y_true)\n",
        "    score = corr/n\n",
        "    return score\n",
        "\n",
        "def load_classifier_data(folder: str = \"nlg-bias\", \n",
        "                         typ: str = \"regard\", \n",
        "                         intrs: str = \"\"):\n",
        "    \"\"\"Load AMT test set.\"\"\"\n",
        "    test_df = pd.read_csv(f\"drive/My Drive/{folder}/data/{typ}/{intrs}test.tsv\", \n",
        "                        sep=\"\\t\", \n",
        "                        header=None)\n",
        "    test = [i[0] for i in test_df.values]\n",
        "    return test\n",
        "\n",
        "def classifier_bootstrap(data_A, data_B, n, R, amt=\"csc699\"):\n",
        "    \"\"\"One-tailed bootstrap for comparing models.\"\"\"\n",
        "    y_true = load_classifier_data(amt)\n",
        "    acc_a = accuracy_score(y_true, data_A)\n",
        "    acc_b = accuracy_score(y_true, data_B)\n",
        "    delta_orig = 2 * (acc_a - acc_b)\n",
        "    cnt = 0\n",
        "    temp_As = []\n",
        "    temp_Bs = []\n",
        "    for r in range(R):\n",
        "        a_temp = random.choices(data_A, k=n) # w/ replacement: bootstrap\n",
        "        b_temp = random.choices(data_B, k=n)\n",
        "        temp_As.append(a_temp)\n",
        "        temp_Bs.append(b_temp)\n",
        "    for ix, (sample_a, sample_b) in enumerate(zip(temp_As, temp_Bs)):\n",
        "        acc_a = accuracy_score(y_true, sample_a)\n",
        "        acc_b = accuracy_score(y_true, sample_b)\n",
        "        delta = acc_a - acc_b\n",
        "        if delta > delta_orig:\n",
        "            cnt += 1\n",
        "    pval = float(cnt)/float(R)\n",
        "    return pval\n",
        "\n",
        "def classifier_permutation_test(data_A, data_B, n, R, amt=\"csc699\"):\n",
        "    \"\"\"Two-tailed permutation test for comparing models.\"\"\"\n",
        "    y_true = load_classifier_data(amt)\n",
        "    acc_a = accuracy_score(y_true, data_A)\n",
        "    acc_b = accuracy_score(y_true, data_B)\n",
        "    delta_orig = np.abs(acc_b - acc_a)\n",
        "\n",
        "    cnt = 0\n",
        "    temp_As = []\n",
        "    temp_Bs = []\n",
        "    \n",
        "    for r in range(R):\n",
        "        indices = random.sample(range(n), n) # without replacement: permutation\n",
        "        temp_As.append([data_A[z] for z in indices])\n",
        "        temp_Bs.append([data_B[z] for z in indices])\n",
        "\n",
        "    for sample_a, sample_b in zip(temp_As, temp_Bs):\n",
        "        delta = np.abs(accuracy_score(y_true, sample_b) - \n",
        "                       accuracy_score(y_true, sample_a))\n",
        "\n",
        "        if delta > delta_orig:\n",
        "            cnt += 1\n",
        "            \n",
        "    pval = float(cnt + 1)/float(R + 1)\n",
        "    return pval\n",
        "\n",
        "def data_permutation_test(data_A: list, \n",
        "                          data_B: list, \n",
        "                          R:int = 10000, \n",
        "                          demo1: str ='gay',\n",
        "                          demo2: str = '',\n",
        "                          main: str = 'A'):\n",
        "    \"\"\"Two-tailed permutation test for comparing datasets.\"\"\"\n",
        "    if not demo2:\n",
        "        # Compare same demographic between datasets A and B.\n",
        "        A = data_A[demo1].tolist()\n",
        "        B = data_B[demo1].tolist()\n",
        "    else:\n",
        "        # Compare pair of demographics within dataset A.\n",
        "        print(f'{main} counts:\\n')\n",
        "        if main == 'A':\n",
        "            print(f'{demo1}: {data_A[demo1].value_counts(normalize=True) * 100}\\n') # [-1] * 100:.1f}...\n",
        "            print(f'{demo2}: {data_A[demo2].value_counts(normalize=True) * 100}')\n",
        "            A = data_A[demo1].tolist()\n",
        "            B = data_A[demo2].tolist()\n",
        "        else:\n",
        "            print(f'{demo1}: {data_B[demo1].value_counts(normalize=True) * 100}\\n')\n",
        "            print(f'{demo2}: {data_B[demo2].value_counts(normalize=True) * 100}')\n",
        "            A = data_B[demo1].tolist()\n",
        "            B = data_B[demo2].tolist()\n",
        "\n",
        "    a = np.mean(A)\n",
        "    b = np.mean(B)\n",
        "    \n",
        "    delta_orig = np.abs(b - a)\n",
        "\n",
        "    cnt = 0\n",
        "    deltas = []\n",
        "    for r in range(R):\n",
        "        pool = np.concatenate((A, B))\n",
        "        np.random.shuffle(pool)\n",
        "        \n",
        "        sample_a = pool[:len(A)]\n",
        "        sample_b = pool[-len(B):]\n",
        "        \n",
        "        sa = np.mean(sample_a)\n",
        "        sb = np.mean(sample_b)\n",
        "        \n",
        "        delta = np.abs(sb - sa)\n",
        "        deltas.append(delta)\n",
        "        if delta > delta_orig:\n",
        "            cnt += 1\n",
        "            \n",
        "    pval = float(cnt + 1)/float(R + 1)\n",
        "    return pval, deltas, delta_orig\n",
        "\n",
        "def get_context_dict(filename: str = \"diff_regard1_small_gpt2_generated_samples.tsv_labeled\"):\n",
        "    \"\"\"Get scores by context, demographic.\"\"\"\n",
        "    demo_df = pd.read_csv(\"drive/My Drive/csc699/demographics.txt\", header=None)\n",
        "    demo_lst = demo_df[0].tolist()\n",
        "    templates = pd.read_csv(\"drive/My Drive/csc699/templates.txt\", sep=\",\")\n",
        "    temp_df = templates.groupby('bias_context').aggregate(lambda x: list(map(lambda y: y[4:], list(x))))\n",
        "    temp_df.reset_index(inplace=True)\n",
        "    respect = temp_df.iloc[1, 1]\n",
        "    occupation = temp_df.iloc[0, 1]\n",
        "    gpt2_df = pd.read_csv(f\"drive/My Drive/nlg-bias/data/generated_samples/regard_diff/{filename}.tsv\", \n",
        "                        header=None, sep=\"\\t\")\n",
        "    dct = {'respect': {k: [] for k in demo_lst}, \n",
        "        'occupation': {k: [] for k in demo_lst}}\n",
        "    for i, j in gpt2_df.values:\n",
        "        for k in respect:\n",
        "            if k in j:\n",
        "                for d in demo_lst:\n",
        "                    if d in j:\n",
        "                        dct['respect'][d].append(i)\n",
        "        for k in occupation:\n",
        "            if k in j:\n",
        "                for d in demo_lst:\n",
        "                    if d in j:\n",
        "                        dct['occupation'][d].append(i)\n",
        "    return dct\n",
        "\n",
        "def get_ratio(samples: list, \n",
        "              polarity: str = 'neg'):\n",
        "    \"\"\"Compute percentage of samples of given polarity.\"\"\"\n",
        "    d = {'pos': 1, 'neu': 0, 'neg': -1}\n",
        "    clss = d[polarity]\n",
        "    s = len([i for i in samples if int(i) == clss])\n",
        "    n = len(samples)\n",
        "    return s/n\n",
        "\n",
        "def class_permutation_test(dct: dict, \n",
        "                           dct2: dict,\n",
        "                           R:int = 10000, \n",
        "                           compare_contexts: bool = False,\n",
        "                           compare_data: bool = False,\n",
        "                           demo1: str ='gay',\n",
        "                           demo2: str = 'straight',\n",
        "                           polarity: str = 'neg',\n",
        "                           context: str = ''):\n",
        "    \"\"\"Two-tailed permutation test.\"\"\"\n",
        "    demoA = f'The {demo1} person' if demo1[-2:] != 'an' else f'The {demo1}'\n",
        "    demoB = f'The {demo2} person' if demo2[-2:] != 'an' else f'The {demo2}'\n",
        "    pd = {'neg': 'negative', 'pos': 'positive', 'neu': 'neutral'}\n",
        "    pr = (lambda p, d, c, s: \n",
        "            f'Ratio of {pd[p].capitalize()} scores for the {d.capitalize()} demographic' +\n",
        "            f' in the {c.capitalize()} context: {s * 100:.1f}')\n",
        "    if not compare_data and compare_contexts:\n",
        "        print(f'Comparing {demo1.capitalize()} between the Respect and Occupation contexts.')\n",
        "        A = dct['respect'][demoA]\n",
        "        B = dct['occupation'][demoA]\n",
        "    elif compare_data and not compare_contexts:\n",
        "        print(f'Comparing \\\"{demoA.capitalize()}\\\" demographic between GPT-2 and LM1B in the {context.capitalize()} context.')\n",
        "        A = dct[context][demoA]\n",
        "        B = dct2[context][demoA]\n",
        "    elif compare_data and compare_contexts:\n",
        "        print(f'Comparing \\\"{demoA.capitalize()}\\\" demographic between GPT-2 and LM1B in both contexts.')\n",
        "        A = dct['respect'][demoA]\n",
        "        A.extend(dct['occupation'][demoA])\n",
        "        B = dct2['respect'][demoA]\n",
        "        B.extend(dct2['occupation'][demoA])\n",
        "    else: # if not compare_data and not compare_contexts\n",
        "        A = dct[context][demoA]\n",
        "        B = dct[context][demoB]\n",
        "\n",
        "    a = get_ratio(A, polarity)\n",
        "    b = get_ratio(B, polarity)\n",
        "    \n",
        "    if not compare_data and not compare_contexts:\n",
        "        print(pr(polarity, demo1, context, a))\n",
        "        print(pr(polarity, demo2, context, b))\n",
        "\n",
        "    delta_orig = np.abs(b - a)\n",
        "    cnt = 0\n",
        "    deltas = []\n",
        "    for r in range(R):\n",
        "        pool = np.concatenate((A, B))\n",
        "        np.random.shuffle(pool)\n",
        "        \n",
        "        sample_a = pool[:len(A)]\n",
        "        sample_b = pool[-len(B):]\n",
        "\n",
        "        sa = get_ratio(sample_a, polarity)\n",
        "        sb = get_ratio(sample_b, polarity)\n",
        "\n",
        "        delta = np.abs(sb - sa)\n",
        "        deltas.append(delta)\n",
        "        if delta >= delta_orig:\n",
        "            cnt += 1\n",
        "            \n",
        "    pval = float(cnt + 1)/float(R + 1)\n",
        "    return pval, deltas, delta_orig\n",
        "\n",
        "def plot_deltas(deltas, \n",
        "                delta_orig, \n",
        "                polarity: str = 'neg', \n",
        "                demo1='gay', \n",
        "                demo2='straight',\n",
        "                context='respect'):\n",
        "    \"\"\"Show differences between samples' and original's test statistic across permutations.\"\"\"\n",
        "    d = {'neg': 'negative', 'pos': 'positive', 'neu': 'neutral'}\n",
        "    delta_orig *= 1\n",
        "    deltas = list(map(lambda x: x * 1, deltas))\n",
        "    p1 = sns.distplot(deltas)\n",
        "    plt.title(f'{demo1.capitalize()}, {demo2.capitalize()}; ' + \n",
        "              f'{context.capitalize()} context; {d[polarity].capitalize()} valence')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.xlabel('$\\delta(X)$')\n",
        "    h = sorted(p1.patches, \n",
        "               key=lambda h: h.get_height())[-1].get_height()\n",
        "    p1.text(delta_orig * .95, \n",
        "            h * .25, \n",
        "            s=f'$\\delta(x)$ = {delta_orig:.1f}', \n",
        "            rotation=0, \n",
        "            horizontalalignment='center', \n",
        "            verticalalignment='center')\n",
        "    p1.axvline(x=delta_orig, \n",
        "               ymax=.2, \n",
        "               color='orange')\n",
        "    plt.savefig(f\"{demo1}_{demo2}_{context}_{polarity}.png\", transparent=True)\n",
        "    plt.show()\n",
        "\n",
        "def get_samples(filename: str = \"diff_regard1_lm1b_generated_samples.tsv_labeled.tsv_scores.txt\"):\n",
        "    with open(\"drive/My Drive/nlg-bias/data/generated_samples/regard_diff/\" + \n",
        "             f\"{filename}\") as infile:\n",
        "        lines = infile.read().splitlines()\n",
        "        df = pd.DataFrame([])\n",
        "        for line in lines:\n",
        "            demo = line[:line.index(\"[\")][:-2]\n",
        "            scores = eval(line[line.index(\"[\"):])\n",
        "            df[demo] = scores\n",
        "    return df\n",
        "\n",
        "def get_context_dict2(filename: str = \"diff_regard1_small_gpt2_generated_samples.tsv_labeled\"):\n",
        "    \"\"\"Get samples, scores by context, demographic.\"\"\"\n",
        "    demo_df = pd.read_csv(\"drive/My Drive/csc699/demographics.txt\", header=None)\n",
        "    demo_lst = demo_df[0].tolist()\n",
        "    templates = pd.read_csv(\"drive/My Drive/csc699/templates.txt\", sep=\",\")\n",
        "    temp_df = templates.groupby('bias_context').aggregate(lambda x: list(map(lambda y: y[4:], list(x))))\n",
        "    temp_df.reset_index(inplace=True)\n",
        "    respect = temp_df.iloc[1, 1]\n",
        "    occupation = temp_df.iloc[0, 1]\n",
        "    gpt2_df = pd.read_csv(f\"drive/My Drive/nlg-bias/data/generated_samples/regard_diff/{filename}.tsv\", \n",
        "                        header=None, sep=\"\\t\")\n",
        "    dct = {'respect': {k: [] for k in demo_lst}, \n",
        "        'occupation': {k: [] for k in demo_lst}}\n",
        "    for i, j in gpt2_df.values:\n",
        "        for k in respect:\n",
        "            if k in j:\n",
        "                for d in demo_lst:\n",
        "                    if d in j:\n",
        "                        dct['respect'][d].append((i, j))\n",
        "        for k in occupation:\n",
        "            if k in j:\n",
        "                for d in demo_lst:\n",
        "                    if d in j:\n",
        "                        dct['occupation'][d].append((i, j))\n",
        "    return dct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OteSdIGAQfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dct = get_context_dict(filename='mine/diff_regard1_my_gpt2_generated_samples.tsv_labeled')\n",
        "dct2 = get_context_dict(filename = 'mine/diff_regard1_my_lm1b_samples.tsv_labeled')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J42NdEmnm3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2 = get_context_dict2(filename='mine/diff_regard1_my_gpt2_generated_samples.tsv_labeled')\n",
        "lm1b = get_context_dict2(filename = 'mine/diff_regard1_my_lm1b_samples.tsv_labeled')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7c4xsVTnppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2_df = pd.DataFrame(gpt2['respect']['The woman'])\n",
        "lm1b_df = pd.DataFrame(lm1b['respect']['The woman'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fj9atTHrJzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gptg = []\n",
        "lmbg = []\n",
        "while len(lmbg) < 5:\n",
        "    lran = random.choice(lm1b_df.values.tolist())\n",
        "    if lran not in lmbg and \"ERROR\" not in lran[1] and \"<\" not in lran[1]:\n",
        "        lmbg.append(lran)\n",
        "while len(gptg) < 5:        \n",
        "    gptg.append(random.choice(gpt2_df.values.tolist()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APZbehU3qmZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm1b_df[lm1b_df[1].str.contains('prostitute')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAgBbn0EsOcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2_df[gpt2_df[1].str.contains('prostitute')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDCBpPLD6IUe",
        "colab_type": "text"
      },
      "source": [
        "Display proportions of demo classes in context:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhS8iBa34VbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(get_ratio(dct['occupation']['The gay person'], 'pos'))\n",
        "display(get_ratio(dct2['occupation']['The gay person'], 'pos'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k1zlyixnW9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dct['occupation']['The gay person']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8OfX0FP1UAg",
        "colab_type": "text"
      },
      "source": [
        "#### Run sig tests:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9UvVMr2bWEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "polarity = 'pos'\n",
        "demo1 = 'woman'\n",
        "demo2 = 'straight'\n",
        "context = 'respect'\n",
        "compare_contexts = False\n",
        "compare_data = True\n",
        "R = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvYT-84U6Bv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pval, deltas, delta_orig = class_permutation_test(dct=dct, \n",
        "                                                  dct2=dct2,\n",
        "                                                  R=R,\n",
        "                                                  context=context, \n",
        "                                                  compare_contexts=compare_contexts,\n",
        "                                                  compare_data=compare_data,\n",
        "                                                  demo1=demo1, \n",
        "                                                  demo2=demo2, \n",
        "                                                  polarity=polarity)\n",
        "if (float(pval) <= float(0.05)):\n",
        "    print(\"\\nSignificant: p-value: {}\".format(pval))\n",
        "else:\n",
        "    print(\"\\nNot significant: p-value: {}\".format(pval))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJhvvOtajm3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, pval = stats.ttest_ind(dct['respect']['The gay person'], \n",
        "                          dct2['respect']['The gay person'])\n",
        "if (float(pval) <= float(0.05)):\n",
        "    print(\"\\nSignificant: p-value: {}\".format(pval))\n",
        "else:\n",
        "    print(\"\\nNot significant: p-value: {}\".format(pval))\n",
        "print('')\n",
        "#stats.ttest_ind(gpt, lmb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH3KieC9QBiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_deltas(deltas, \n",
        "            delta_orig, \n",
        "            demo1=demo1, \n",
        "            demo2=demo2, \n",
        "            polarity=polarity, \n",
        "            context=context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVZvcI2g_X7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_p = [t[0] for t in pd.read_csv(f\"drive/My Drive/csc699/models/regard/custom/test_predictions.txt\", \n",
        "                                    header=None, \n",
        "                                    sep=\"\\t\").values.tolist()]\n",
        "with open(f\"drive/My Drive/csc699/checkpoints/test_predictions_e_mine.txt\") as lstm_file:\n",
        "    lstm_p = eval(lstm_file.read().splitlines()[0])\n",
        "bert_bp = [t[0] for t in pd.read_csv(f\"drive/My Drive/csc699/models/regard/custom/mine/test_predictions.txt\", \n",
        "                                     header=None, \n",
        "                                     sep=\"\\t\").values.tolist()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF2PmRO8h_H4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pval = classifier_permutation_test(bert_bp, \n",
        "                                   lstm_p, \n",
        "                                   len(bert_bp), \n",
        "                                   20000, \n",
        "                                   amt=\"csc699\")\n",
        "\n",
        "if (float(pval) <= float(0.05)):\n",
        "    print(\"\\nSignificant: p-value: {}\".format(pval))\n",
        "else:\n",
        "    print(\"\\nNot significant: p-value: {}\".format(pval))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HEghfc0LrOE",
        "colab_type": "text"
      },
      "source": [
        "#### Sample scores exploration (GPT-2, LM1B after classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N002-S39Fx2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lmb = get_samples(\"diff_regard1_lm1b_generated_samples.tsv_labeled.tsv_scores.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bl82OkaHlMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt = get_samples(\"diff_regard1_small_gpt2_generated_samples.tsv_labeled.tsv_scores.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXQEv7TCSN6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(gpt.describe())\n",
        "lmb.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VszPdzA9t3Ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, pval = stats.ttest_rel(gpt.gay, gpt.straight)\n",
        "if (float(pval) <= float(0.05)):\n",
        "    print(\"\\nSignificant: p-value: {}\".format(pval))\n",
        "else:\n",
        "    print(\"\\nNot significant: p-value: {}\".format(pval))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeJPy-2Dt7Da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, pval = stats.ttest_ind(gpt.gay, gpt.straight)\n",
        "if (float(pval) <= float(0.05)):\n",
        "    print(\"\\nSignificant: p-value: {}\".format(pval))\n",
        "else:\n",
        "    print(\"\\nNot significant: p-value: {}\".format(pval))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1UU7mSGP7-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "demo1 = 'gay'\n",
        "demo2 = 'straight'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF7xqpY9Mpfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pval, deltas, delta_orig = data_permutation_test(gpt, \n",
        "                                                 lmb, \n",
        "                                                 R=10000, \n",
        "                                                 demo1=demo1, \n",
        "                                                 demo2=demo2, \n",
        "                                                 main='A')\n",
        "\n",
        "if (float(pval) <= float(0.05)):\n",
        "    print(\"\\nSignificant: p-value: {}\".format(pval))\n",
        "else:\n",
        "    print(\"\\nNot significant: p-value: {}\".format(pval))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDyCPdpvXGbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, pval = stats.ttest_ind(gpt[demo1], lmb[demo1])\n",
        "if (float(pval) <= float(0.05)):\n",
        "    print(\"\\nSignificant: p-value: {}\".format(pval))\n",
        "else:\n",
        "    print(\"\\nNot significant: p-value: {}\".format(pval))\n",
        "print('')\n",
        "stats.ttest_ind(gpt, lmb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX6fo9GlfM6b",
        "colab_type": "text"
      },
      "source": [
        "# VADER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LK91hmIUwB9",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZg4DL31RBhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY-vsP3JroAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random, copy\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR1xSPDypKuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIfm4WTjUyad",
        "colab_type": "text"
      },
      "source": [
        "### Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg2UgdUEQv7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "analyser = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqp_3OTgUoGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = [\"Black\", \"White\", \"Man\", \"Woman\", \"Gay\", \"Straight\"]\n",
        "\"\"\"\n",
        "indices = [\"Black Woman\", \"White Woman\", \"Black Man\", \"White Man\", \n",
        "            \"Gay Man\", \"Lesbian\", \"Straight Man\", \"Straight Woman\", \n",
        "            \"Black Lesbian\", \"White Lesbian\", \"Gay Black Man\", \"Gay White Man\",\n",
        "            \"Straight Black Woman\", \"Straight White Woman\",\n",
        "            \"Straight Black man\", \"Straight White Man\"]\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnYM304LREtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sentiment(sentences) -> None:\n",
        "    \"\"\"Compute compound scores and label heuristically.\"\"\"\n",
        "    l = map(lambda s: analyser.polarity_scores(s)['compound'], sentences)\n",
        "    labels = []\n",
        "    for i, p in enumerate(l):\n",
        "        if p >= 0.05:\n",
        "            labels.append(1)\n",
        "        elif p <= -0.05:\n",
        "            labels.append(-1)\n",
        "        else: # -0.05 < p < 0.05\n",
        "            labels.append(0)\n",
        "    return [\" \".join((s, str(l))) for s, l in zip(sentences, labels)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKUpneo8WFgl",
        "colab_type": "text"
      },
      "source": [
        "### Read, annotate and write files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY6FNldmUuPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"drive/My Drive/csc699/demographics.txt\") as demof: # intersectional.txt\n",
        "    demos = demof.read().splitlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyGwhDjHQihA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_samples_df = pd.read_csv(\"drive/My Drive/csc699/cleaned_samples.csv\", # cleaned_samples_int\n",
        "                   converters={\"Respect\": lambda x: eval(x), \n",
        "                               \"Occupation\": lambda x: eval(x)})\n",
        "cleaned_samples_df_xyz = pd.read_csv(\"drive/My Drive/csc699/cleaned_samples.XYZ.csv\", # cleaned_samples_int.XYZ\n",
        "                      converters={\"Respect\": lambda x: eval(x), \n",
        "                                  \"Occupation\": lambda x: eval(x)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOg-23Lva5V6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_samples_df.insert(0, column=\"Labels\", value=0)\n",
        "cleaned_samples_df_xyz.insert(0, column=\"Labels\", value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_aVxqZ4bA3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contexts = [\"Respect\", \"Occupation\"]\n",
        "col_df = pd.DataFrame(columns=[\"Demographic\", \"Respect\", \"Occupation\"])\n",
        "col_df.Demographic = cleaned_samples_df.copy().Demographic\n",
        "for ix, d in enumerate(indices):\n",
        "    for c in contexts:\n",
        "        lst = copy.deepcopy(cleaned_samples_df.loc[ix, c])\n",
        "        lst = get_sentiment(lst)\n",
        "        col_df.loc[ix, c] = lst\n",
        "        \n",
        "cdf_xyz = pd.DataFrame(columns=[\"Demographic\", \"Respect\", \"Occupation\"])\n",
        "cdf_xyz.Demographic = cleaned_samples_df_xyz.copy().Demographic\n",
        "for ix, d in enumerate(indices):\n",
        "    for c in contexts:\n",
        "        lst = copy.deepcopy(cleaned_samples_df_xyz.loc[ix, c])\n",
        "        lst = get_sentiment(lst)\n",
        "        cdf_xyz.loc[ix, c] = lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOqZ4GJVbMo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# col_df.to_csv(\"drive/My Drive/csc699/VADER.csv\", index=False) # VADER_int\n",
        "# cdf_xyz.to_csv(\"drive/My Drive/csc699/VADER_xyz.csv\", index=False) # VADER_int_xyz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXxeCMNrbPsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_col_df = pd.read_csv(\"drive/My Drive/csc699/VADER.csv\", # VADER_int\n",
        "                      converters={\"Respect\": lambda x: eval(x), \n",
        "                                  \"Occupation\": lambda x: eval(x)})\n",
        "test_col_df_xyz = pd.read_csv(\"drive/My Drive/csc699/VADER_xyz.csv\", # VADER_int_xyz\n",
        "                      converters={\"Respect\": lambda x: eval(x), \n",
        "                                  \"Occupation\": lambda x: eval(x)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TxV_boQWvHR",
        "colab_type": "text"
      },
      "source": [
        "# Write files for MTurk annotation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tt6_uKxaGDa2",
        "colab": {}
      },
      "source": [
        "with open(\"drive/My Drive/csc699/to_ano_int2.csv\", \"w\") as outtrans:\n",
        "    outtrans.write(\"text\\n\")\n",
        "    for i in p:\n",
        "        outtrans.write(i + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0IEz8CaEGDa6",
        "colab": {}
      },
      "source": [
        "with open(\"drive/My Drive/csc699/to_ano_int2.XYZ.csv\", \"w\") as outtrans:\n",
        "    outtrans.write(\"text\\n\")\n",
        "    for i in x:\n",
        "        outtrans.write(i + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sl5J0u1xGDa8",
        "colab": {}
      },
      "source": [
        "with open(\"drive/My Drive/csc699/to_ano_int2_labs.XYZ.csv\", \"w\") as outtrans:\n",
        "    outtrans.write(\"text\\n\")\n",
        "    for i in labels:\n",
        "        outtrans.write(i + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w1A28JBwGDa9",
        "colab": {}
      },
      "source": [
        "with open(\"drive/My Drive/csc699/to_ano_int2_xyzlab.csv\", \"w\") as outtrans:\n",
        "    outtrans.write(\"text\\n\")\n",
        "    for i, x in zip(labels, x):\n",
        "        outtrans.write(f\"{i} {x}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORPIuam1NZL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"with open(\"drive/My Drive/csc699/to_annotate.csv\", \"w\") as to_ano:\n",
        "    for sent, label in xyz_sentences:\n",
        "        to_ano.write(sent + \"\\n\")\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJawdTiGSSWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"with open(\"drive/My Drive/csc699/to_annotate_int.csv\", \"w\") as to_ano:\n",
        "    for sent, label in xyz_sentences:\n",
        "        to_ano.write(sent + \"\\n\")\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFHEnWcH6OuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"drive/My Drive/csc699/to_annotate_int_corr.csv\", \"w\") as to_ano:\n",
        "    for sent, label in poldf_sentences:\n",
        "        to_ano.write(sent + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVAwzqLpSWGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"with open(\"drive/My Drive/csc699/to_ano_labs.csv\", \"w\") as to_ano:\n",
        "    for sent, label in xyz_sentences:\n",
        "        to_ano.write(label + \"\\n\")\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuTUPAACNZ5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"with open(\"drive/My Drive/csc699/to_ano_int_labs.csv\", \"w\") as to_ano:\n",
        "    for sent, label in xyz_sentences:\n",
        "        to_ano.write(label + \"\\n\")\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePyDq7CbPDUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xyzdf.to_csv(\"drive/My Drive/csc699/xyzsampled.csv\", index=False) # xyzsampled_int"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}